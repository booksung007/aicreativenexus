2024.05.02
I have chosen to go with the local AI install. Before starting to install local AI lets look over some pros and cons of installing local AI. If you want to skill to intalling procedure right away chick here. 
Some pros of local AI model. Depenting on the model. 

Some thing to now before running the model locally. First there is the concept of backend and frontend and the model. Model is the actual file that is trained in LLM, I am not completly sure how this small GB file can become an AI model but this file will be doing all the work. Backend is the program that will run the model in your computer. Finally the front end is the GUI of the AI. 

First thing first we need a backend to run the AI model. There is few options for this one is oobabooga a text generation. This is a guide I follow using the guide from youtube. I have a windows operation system so this example will be in windows setting. 

In order to install oobabooga first go to 
1. Go to Github follow this link:
2. Download the file by pushing the button on the right. 
3. After you downloaded the file unzip the file using 7zip.(not sure) Be sure to unzip the files in folder with no spaces and it is a safe bet to unzip the file in English named folder. My native language is not English and I had some Korean named folder which created error. 
4. Go to the folder you unzipped the folder and start the 'start_window.bat' file. This file will pull up an cmd window.(is this the case?) 
5. On the window it will ask you what kind of graphic card you have. I have a nvidea graphic card and chose accordingly.
6. After the instalation is complet a number in a website format will appear. Copy this URL and post it on to your browser of chosing. 
7. This will open up the oobabooga web UI.

source: youtube video: Aitreprenur

Congratulation you have succesfully installed the backend 'oobabooga text generator'. Now we have to upload the 

There are many differnet models and new models keep poping up every month. Which is exciting. However you should first consider if your computer can actually run the model. Spec check if you will.

After you finish all this now you can start playing with the AI! Ofcourse you can add more before 
continuing on. Frontend is more extended of the backend a program to help you interact with the LLM. Frontend is optional and not strictly limited to local model. Frontend can be connected to online models. You can check how to install front end by reading this post


Thats it! You have succesfully installed a local model in your computer. The world is your oyster. Have fun!


source for this matirial: https://rentry.org/cixpvn93
Found on 4chan /vg/ /aids/

//draft1

Before running the local model I had to look around the web for many different concepts and functions. In this guide I will share my findings and explain to you how to run a AI model locally. 

There is three componant in local model. First there is the model itself. The file that contain the LLM. This can be downloaded from the site hugggingface.com. There is lots of different models. If you go to huggingface.com it will be quite overwhelming. I know I was. So read this page to get a barring of the site and how to choose between different models. 

However one can't run a AI program with only this raw file you need a program to run the program. That where backend comes to play. Backend is a program that run the LLM file on your computer. There are also different backend programms to choose from. I personally choose 'oobaboga text ...' But others seem to recommend different methods. 

Finally there is the frontend. This program as the name suggest goes on top of the backend and give you many differnt ways to interact with the local model. Such as providing 'lore book' and other nifty options for you conviniance. 

(picture of layered model-backend-frontend-user)

That was a bref(?) explanation of what you need to run the local model. So now the task is how to actually install and run the whole thing. 

That was long was it so where should you start? Well not yet. You still haven't checked if you can actually run the model. Your hardware spec. So as I checked I think the best way to approch this gordian kont is start by step by step.

1. Check your hardware specification.
2. Install the backend software. 
3. Download the LLM model.
4. Install the frontend software.(optional)

I think this is the most straight forward way to get things started. You can skip the steps if you already know what each step do. Such as if you already know your hardware specification you can skip right to installing the backend software. However I suggest at least skimming thourght each step because there might be some information you might not be aware. 

//draft2

Before running the local text generation model I had to look around the web to understand the basics. In this guide I will share my findings and explain 3 basic componant needed to run a text generation AI model locally. 

There are three componants in a text genertion local model. The model itself, backend program, front end program. 

First there is the model itself. The file that contain the LLM itself. This can be downloaded from the site hugggingface.com. A platfrom focused on machine learning. When you enter huggingface.com there are lots of different models too choose rom, too many...... it will be quite overwhelming. I know I was. So read this page to get a bearing of the site and how to choose between different models. 

Understanding LLM in Huggingface.com.

One can't run a AI model with only LLM file, you need a program to run the LLM. That where backend comes to play. Backend is a program that run the LLM file on your computer. There are many different backend programms to choose from. I personally choose 'oobaboga text-generation-webui' But there are other backend program each with different perks. 

Setting up the backend for local text generation model. 

Finally there is the frontend. This program goes on top of the backend and give you many differnt ways to interact with the LLM. Such as providing 'lore book' and other nifty options for you conviniance. 

Setting up the frontend for local text generation model. 

(picture of layered model-backend-frontend-user)

This is my finding of what is needed to run text generation AI locally. Now knowing the ingredients the task is how to actually install and run the whole thing. So as I checked I think the best way to approch this gordian kont is start by step by step.

1. Check your hardware specification.
2. Install the backend software. 
3. Download the LLM model.
4. Install the frontend software.(optional)

I think this is the most straight forward way to get things started. You can skip the steps if you already know what each step do. Such as if you already know your hardware specification you can skip right to installing the backend software. However I suggest at least skimming thourght each step because there might be some information you might not be aware. 

//draft 3
Read time: 2min 372 word
Before running the local text generation model, I had to search the web to grasp the basics. In this guide, I will share my findings and explain the three basic components needed to run a text generation AI model locally.

There are three components in a text generation local model: the model itself, backend program, and frontend program.

First, there is the model itself: the file containing the LLM. This can be downloaded from the site huggingface.com, a platform focused on machine learning. When you visit huggingface.com, you'll find numerous models to choose from, which can be overwhelming. So, I suggest reading this page to get a sense of the site and how to select between different models.

[[04. Understanding LLM in Huggingface.com]]

You can't run an AI model with only an LLM file; you need a program to run the LLM. That's where the backend comes into play. The backend is a program that runs the LLM file on your computer. There are many different backend programs to choose from. Personally, I chose 'oobaboga text-generation-webui,' but there are other backend programs, each with different perks.

Setting up the backend for local text generation model.

Finally, there's the frontend. This program goes on top of the backend and provides you with various ways to interact with the LLM, such as providing a 'lore book' and other convenient options.

Setting up the frontend for local text generation model.

Insert Picture of Layered Model: Backend - Frontend - User

These are my findings on what is needed to run a text generation AI locally. Now, knowing the ingredients, the task is figuring out how to actually install and run the whole thing. So, after checking, I believe the best approach is to tackle this Gordian knot step by step.

1. Check your hardware specifications.
2. Install the backend software.
3. Download the LLM model.
4. Install the frontend software. (optional)

I think this is the most straightforward way to get things started. You can skip steps if you already know what each one does. For example, if you already know your hardware specifications, you can skip right to installing the backend software. However, I suggest at least skimming through each step because there might be some information you're not aware of.