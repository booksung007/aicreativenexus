//draft4 ChatGPT3.5 
When I first visited 'huggingface.com', I was met with a plethora of uploaders and even more models to choose from, which was quite overwhelming. In this post, I will share my experience navigating the site and guide you on how to find and run a local model.

The first step is finding a reputable uploader. Based on a YouTube video recommendation (source below), I opted for an uploader called 'TheBloke'. 'TheBloke' appears to be a well-regarded uploader on huggingface.com, with numerous models uploaded, many downloads, and a high number of upvotes.

Once I had selected the uploader, I needed to choose the specific model I wanted to run. A typical model title on huggingface.com looks like this:

TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ 
TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF 
TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ

At first glance, the file names may seem long and daunting. Here's how I deciphered them: the most crucial information is the number and letter '7B'. This number indicates the size of the model's parameters and thus its complexity. Larger numbers signify more advanced models. These models come in various sizes, such as 7B, 13B, 30B, 45B, 70B, 100B, and larger. As the numbers increase, so does the computing power required to run the model. The key hardware specification to consider is VRAM. Typically, a GPU with 12GB VRAM can handle 13B models, while 24GB VRAM is suitable for 30B, 45B, and 70B models (source below).

Next, there are the letters GPTQ, GGUF, and AWQ in the title. According to information I gathered from a YouTube video, these letters denote how the model is compressed, making it possible to run the model on standard personal computers. GGUF is the format used when running the model strictly from your CPU, while GPTQ and AWQ both utilize the GPU for model execution.

I have an 'Nvidia 4070ti', which has a VRAM of approximately 11996MB, just shy of 12GB. Initially, I attempted to run a '13B' parameter 'AWQ' model uploaded by 'TheBloke', but encountered an error with the text generator. I assumed it would be possible to run the '13B' 'AWQ' model since I was only slightly below the 12GB VRAM limit. However, it appears that surpassing the limit is necessary to successfully execute the model.

The model that worked for me was a '7B', 'AWQ' model. I successfully ran 'TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ' on my local computer. Subsequently, I tested 'KoboldAI/Mixtral-7B-Holodeck-1', a community-made model focused on writing fiction and role-playing. This model is based on e and was fine-tuned for fiction and role-playing. I am still uncertain about the system requirements for running individual models, particularly how much GPTQ models offload the GPU load to the CPU and RAM. Further experimentation with different models is needed to gain a better understanding.

How to check your PC specification and VRAM.(under construction.) 

Referance website: 
https://rentry.org/cixpvn93
Referance Youtube video: 
https://www.youtube.com/watch?v=C-7jGYOGvy4&t=422s&ab_channel=Aitrepreneur

//draft3
When opening up 'huggingface.com' I was confronted with numerous uploaders and even more models to choose from. It was quite overwhelming. In this post I will explain how I navigated through the site and show you how to find a local model you can run. 

First step is finding a reputable uploader. I was recommanded the uploader called 'TheBloke' from a youtube video(source below). 'TheBloke' seem to be a reputable uploader with lots of models uploaded on huggingface.com with lots of downloads and many up votes.

After I choosing which uploader to download your model from I had to choose the model I wanted to run. Here is what a typical model title will look like on huggingface.com.

TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ
TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF
TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ

At first glance the file name is long and daunting. Here is how I understand it. The most important information is the number and letter '7B'. The number indecates how large of a parameter the model is trained. The larger the number the 'smarter' the model. There are 7B, 13B, 30B, 45B, 70B, 100B and larger models. As the numbers go higher the more computing power needed to run the model. The most important hardware number you are looking for is VRAM. Typically 12GB VRAM alow you to run 13B models. If you have less you have to run lower number models. 24GB can run 30B, 45B and 70B models. (source below)

Next there is the GPTQ, GGUF, AWQ part of the title. From information I gathered from a youtube video these letters means how the model is compressed, making it possiable to run the model on commercial grade personal computers. GGUF is the fromat you use when you want to run the model strictly from your CPU. GPTQ and AWQ is both similar that use GPU for running the model.  

I have a 'Nvida 4070ti' which has a had VRAM of 11996mb roughtly little less than 12GB. I first tried to run a '13B' parameter 'AWQ' model uploaded by 'TheBlock' but the oobaboga text generator returned a error. I thought it was possiable to run the '13B' 'AWQ' model because I was only tad bit shy of 12gb of VRAM. However it seem you have to be over the limit to actually run the model. 

Model that worked was '7B', 'AWQ' model. I was able to run 'TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ' on my local computer. I then tried 'KoboldAI/Mixtral-7B-Holodeck-1'. KoboldAI is a community made model focused on writing fiction and roleplaying. The model is based on  e and was finetuned to write fiction and roleplaying. I am still not sure the system requirement of running individual model. Especally how much GPTQ model off load the GPU load to CPU and RAM. I need to try more models and see how they work exactly.  

How to check your VRAM. 

Referance website: 
https://rentry.org/cixpvn93
Referance Youtube video: 
https://www.youtube.com/watch?v=C-7jGYOGvy4&t=422s&ab_channel=Aitrepreneur
https://www.youtube.com/watch?v=XwL_cRuXM2E&t=175s&ab_channel=bycloud

//draft2
When opening up 'huggingface.com' I was confronted with numerous uploaders and even more models to choose from. I was overwhelmed by it at first. In this post I will explain how I navigated through the site and show you how to find a local model you can run. 

First step is finding a reputable uploader. I was recommanded the uploader called 'TheBloke' from a youtube video. He seem to be a reputable uploader with lots of models uploaded on huggingface.com with lots of downloads and many up votes. 

After I choose which uploader to choose from I had to choose the model I wanted to run. Here is what a typical model title will look like on huggingface.com. 

TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ
TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF
TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ

At first glance the file name is long and looks convoluted. Here is how I understand it. The most important information is the number and letter '7B'. The number indecates how large of a parameter the model is trained. The larger the number the 'smarter' the model. There are 7B, 13B, 30B, 45B, 70B, 100B and larger models. As the numbers go higher the more computing power needed. The most important hardware number you are looking for is VRAM. Typically 12GB VRAM alow you to run 13B models. If you have less you have to run lower number models. 24GB can run 30B, 45B and 70B models. (source below)

Next there is the GPTQ, GGUF, AWQ part of the title. From information I gathered from a youtube video(source below) these letters means how the model is compressed, making it possiable to run the model on commercial grade personal computers. GGUF is the fromat you use when you want to run the model strictly from your CPU. GPTQ and AWQ is both similar that use GPU for running the model.  

I have a 'Nvida 4070ti' not a bad graphics card by any means. It could run any game I wish to play with pretty much no hiccup. However it only had VRAM of 11996mb roughtly little les than 12GB. I first tried to run a 13B model but the oobaboga text generator returned a error. I was kind of disappointed that my 'Nvida 4070ti' counldn't run a 13B parameter AWQ model. I thought it was possiable because it was only tad bit shy of 12gb of VRAM. Model that worked was 7B, AWQ model.  I then tried 'KoboldAI/Mixtral-7B-Holodeck-1' which was finetuned on using Mistral's 7B model which I could actually run it on my computer! I am still not sure the system requirement of running individual model. Especally how much GPTQ model off load the load to CPU and RAM. More experiment is needed. 

How to check your VRAM. 

Referance website: 
https://rentry.org/cixpvn93
Referance Youtube video: 
https://www.youtube.com/watch?v=C-7jGYOGvy4&t=422s&ab_channel=Aitrepreneur

//draft1 
When opening up 'huggingface.com' numerous uploaders and even more models to choose from. I was overwhelmed by it at first. In this post I will explain how I navigated through the site and show you how to find the local model just right for yourself. 

First step is finding a reputable uploader. I was recommanded the uploader called 'TheBloke' from a youtube video. He seem to be a reputable uploader with lots of models uploaded on huggingface with lots of downloads and many up votes. 

Now lets look at the indivisual models. 

TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ
TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF
TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ

//I don't know any of these cut out all the part where I don't know or can't explain. Just write what I know and what I can explain with my own words and what I have experianced first hand. 

//Question, If 'CapybaraHermes' is the name what does 'Mistral' actually means? I know that it's a company name but what does it actually mean in the context? Did 'Mistral' made this model? If so what does the 'theblock' do with this model? 

These are LLM models 'TheBloke' uploaded. Here is how I understand it. 'CapybaraHermes' is the name of the models. Under the read me section you can read about what the models are about, how it is trained and what kind of task it excels at. '2.5' means the version of the model. 'Mistral' is the company of the model made with which I am not sure how that works. 

Now here is the important information. '7B' means how large is the parameters size is the model. The number indecates how big of a parameter the model is trained. There are 7B, 13B, 30B, 45B, 70B, 100B and larger models. The larger the number the 'smarter' the model. The most important hardware number you are looking for is VRAM. Typically 12GB VRAM alow you to run 13B models. If you have less you have to run lower number models. 24GB can run 30B, 45B and 70B models. 

Finally there is the GPTQ, GGUF, AWQ part of the title. From what I understand these end format means the model is compressed to let you help run the model run on your computer with out hindering the AI model. GPTQ is the old version of the GGUF so it is better to use 
GGUF is the fromat you use when you want to run the model strictly from your CPU. GPTQ and AWQ is both similar that use GPU for running the model.  

I have a 'Nvida 4070ti' not a bad graphics card by any means. It could run any game I wish to play with pretty much no hiccup. But it only have ** VRAM so the oobaboga text generator returned a error. I was kind of disappointed that my 'Nvida 4070ti' counldn't even run a 7B parameter model. I then tried 'KoboldAI/Mixtral-7B-Holodeck-1' which was finetuned on using Mistral's 7B model which I could actually run it on my computer! I am still not sure how this work 


Referance: 
https://rentry.org/cixpvn93
Youtube video: 
https://www.youtube.com/watch?v=C-7jGYOGvy4&t=422s&ab_channel=Aitrepreneur

//first draft
2024.05.02
When you Open up Huggingface there are thousands of uploaders and even more models to choose from. I was overwhelmed by it. You should try different models for youself but lets look at a simple example. 

Here is a reputable uploader 'TheBloke' a reputable uploader with many different models that has thousands of downloads. If you look at them all models have different information in there name. Lets look at one example. 

TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ
TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF
TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ

These are LLM model 'TheBloke' uploaded. 'CapybaraHermes' is the name of the model. Under the read me section you can read about what the model is about, how it is trained and what kind of task it excels at. '2.5' means the version of the model. 'Mistral' is the company of the model made. Now here is the important information. '7B' means how large is the parameters size is the model. The number indecate how big the model is trained. There is 7B, 13B, 30B, 45B, 70B, 100B larger. 
The larger the number the 'smarter' the model. The most important hardware number you are looking for is VRAM. 

[[XX. Hardware requirement for running Local AI model.]]

Typically 12GB VRAM alow you to run 13B models. If you have less you have to run lower number models. 24GB can run 30B, 45B and 70B models. 

The way you can fit those models inside your VRAM even though they are much bigger is their raw state is via the compression process called quantization. 
Quantization allows to compress the modles to smaller sizes with only small losses Q8_0 makes the model's size in GB to roughly crrespond to its Billion of prameters, with numbers lower than 8 representing further compression. At this quant, there is no preceptible diffeence between raw and quantized model. Q4_K_M is considered the point past which model's degradation is starting to become worse and much more noticeable. Extreme quantization like Q2_K is advisded against, but you cna always experiment with a model you really like and see if responses are satisfactory for your needs. Even though Q4_K_M is used the minimum, it doesn't mean that Q3_K_S is unusable, even if it's a little dumb. 

With the above tow paragraphs in mind, the simplest way to decide on your hardware requirements is to check if Q4_K_M version of the model fits into your VRAM(allow some leeway for context size). If not, you can either go with smaller model or lower quant: the choice is yours.

The best value GPU you cna buy is used 3090, with 3060 being the second best for cases of abject poverty. Don't let the others to cloud your judgemtn with cheaper old cards and AMD 32 GB GPUs - they are not worth it. Buy a used 3090 from ebay or another marketplace and enjoy. 
(This is plagerism I need to change this)

Finally lets look at the last GPTQ, GGUF, AWF in the last part of the title. 

Source: https://rentry.org/cixpvn93
Youtube video: 
https://www.youtube.com/watch?v=C-7jGYOGvy4&t=422s&ab_channel=Aitrepreneur